# deep-learning-optimizer-comparison

A comparative study of deep learning optimization algorithms using a simple neural network. Evaluates Gradient Descent variants and adaptive optimizers on classification performance using accuracy and F1 score.

- **Choose an appropriate problem** and solve it using Deep Learning (use a simple network).
- **Use visualization tools** as much as possible.
- **Presentation is required.**

### Evaluation Metrics

- **Accuracy**
- **F1 Score**

### Objective

Compare the performance of the optimization algorithms in solving the selected problem.

### Experimental Setup

- The experimental setup should be **identical for each group of algorithms**.

### Comparisons to Make

1. **Part-I algorithms**
2. **Part-II algorithms**
3. **Part-III algorithms**
4. **The best algorithm from each group**

Finally, **rank all 7 algorithms** based on their performance.

---

### Expected Deliverables

- **Running, error-free code** with sufficient visualization and important output screens.
- **Report** sufficiently describing your implementation (could be a slide or a PDF).
- **Do not zip the files**; submit them as they are.

---

### Algorithm Groups

#### Part-I

1. Gradient Descent (GD/Batch GD)
2. Minibatch SGD
3. Stochastic Gradient Descent

#### Part-II

4. Gradient Descent
5. Gradient Descent with Momentum
6. Gradient Descent with Nesterov Momentum

#### Part-III

7. AdaGrad
8. RMSProp
9. Adam

```
